{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop V/S Spark\n",
    "\n",
    "| Parameter       | Hadoop                                                                                        | Spark                                                                                                         |\n",
    "|-----------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| Performance     | Hadoop is slower then spark. it writes data back to the disk and read again from to in-memory | Spark is faster then hadoop because spark do all the the computation in memory.                               |\n",
    "| Batch/Streaming | Build for batch data processing.                                                              | Build for batch as well as streaming data processing.                                                         |\n",
    "| Ease Of Use     | Difficult to write code in hadoop. Hive was built to make it easier                           | Easy to write and debug code. Interactive shell to develop and test. Spark provides high and low level API's. |\n",
    "| Security        | Use kerberos Authentication and ACL autirization. (YARN)                                      | Don't have solid security. (HDFS->ACL)(YARN->kerberos).                                                       |\n",
    "| Fault Talerance | It has block of data (128 MB) and replication factor to handle the failure.                   | Use DAG to provide fault talerance (DAG).                                                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data In Spark\n",
    "1. Format (Optional) -> CSV, JSON, JDBC/ODBC, Table, parquate. If you do not define any reading method, then by default it takes parquate as read method.\n",
    "2. Option (Optional) -> InferSchema, Mode, header.\n",
    "3. Schema (Optional) -> Custom schema can be used.\n",
    "4. Load -> File path.\n",
    "\n",
    "Read Mode:\n",
    "1. Failfest: Fail execution if malformed record in dataset.\n",
    "2. Dropmalformed: Drop the corrupted record.\n",
    "3. Permissive: Default mode. Set null values to all the corrupted fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as Fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSumbittest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"true\")\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|\n",
      "|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|\n",
      "| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|\n",
      "| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|\n",
      "| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Job Submit (Assumptions DriverMemory-20GB, TotalExecutor-5, ExeutorCores-4, ExecutorMemory-25GB )\n",
    "1. Master node first create Driver in any worker node.\n",
    "2. Driver which is also known as Application Driver. Spark is writen in scala, and scala is a JVM process. Inside the Driver container it will create 2 main methods, one is for pyspark and another is for JVM.  Spark Core -> Java Wrapper -> Python Wrapper. JVM is called Application driver and pyspark is called pyspark driver.\n",
    "3. Then driver check the executor details and then it send the request to the resource manager. \n",
    "4. The resource manager sent request to node manager (worker), then it creates 5 executors in the ideal workers.\n",
    "5. Application driver send data and other details to the executors for the processing.\n",
    "6. All the excutors send computated result tot the driver.\n",
    "7. In the end all the container driver & exector will be delete. \n",
    "\n",
    "Note: Avoid writing/using UDF funtion in the pysaprk, it will require python worker in the executor container so it will impact on the performace. Always use buit-in function.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saprk Schema: There are 2 types of schrma\n",
    "1. Using StructType & StructField\n",
    "    i. StructType: Defines structure of dataframe. List of StructField\n",
    "    ii. StructField: Define the column data type.\n",
    "    Example: StructType([StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)])\n",
    "2. Using DDL: In quotes comma seperated columns with data type in space.\n",
    "    Example: \"id integer, name string, age integer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_schema = StructType([StructField(\"airline\", StringType(), True), StructField(\"flight\", StringType(), True), \\\n",
    "                            StructField(\"source_city\", StringType(), True), StructField(\"departure_time\", StringType(), True), \\\n",
    "                            StructField(\"stops\", StringType(), True), StructField(\"arrival_time\", StringType(), True), \\\n",
    "                            StructField(\"destination_city\", StringType(), True), StructField(\"class\", StringType(), True), \\\n",
    "                            StructField(\"duration\", IntegerType(), True), StructField(\"days_left\", IntegerType(), True), StructField(\"price\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- source_city: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stops: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- days_left: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300153"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65102"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\")    # Trasormation\n",
    "df1.count()     # Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When action is called then only trasformation is triggered.\n",
    "\n",
    "Types Of Trasformations\n",
    "1. Narrow -> Trasformation that doesn't require data movement within partition. Eg. Filter, Select, Union, Map\n",
    "2. Wide -> Trasformation that require datat movement within partition. Eg Join, GroupBy, Distinct. In wide trasfomation need to do data suffling between the partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
