{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hadoop V/S Spark\n",
    "\n",
    "| Parameter       | Hadoop                                                                                        | Spark                                                                                                         |\n",
    "|-----------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| Performance     | Hadoop is slower then spark. it writes data back to the disk and read again from to in-memory | Spark is faster then hadoop because spark do all the the computation in memory.                               |\n",
    "| Batch/Streaming | Build for batch data processing.                                                              | Build for batch as well as streaming data processing.                                                         |\n",
    "| Ease Of Use     | Difficult to write code in hadoop. Hive was built to make it easier                           | Easy to write and debug code. Interactive shell to develop and test. Spark provides high and low level API's. |\n",
    "| Security        | Use kerberos Authentication and ACL autirization. (YARN)                                      | Don't have solid security. (HDFS->ACL)(YARN->kerberos).                                                       |\n",
    "| Fault Talerance | It has block of data (128 MB) and replication factor to handle the failure.                   | Use DAG to provide fault talerance (DAG).                                                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV Data In Spark\n",
    "1. Format (Optional) -> CSV, JSON, JDBC/ODBC, Table, parquate. By default it takes parquate as read method.\n",
    "2. Option (Optional) -> InferSchema, Mode, header.\n",
    "3. Schema (Optional) -> Custom schema can be used.\n",
    "4. Load -> File path.\n",
    "\n",
    "Read Mode:\n",
    "1. Failfest: Fail execution if malformed record in dataset.\n",
    "2. Dropmalformed: Drop the corrupted record.\n",
    "3. Permissive: Default mode. Set null values to all the corrupted fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as Fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/08 00:14:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSumbittest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|\n",
      "|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|\n",
      "| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|\n",
      "| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|\n",
      "| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"true\")\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Job Submit \n",
    "\n",
    "(Assumptions DriverMemory-20GB, TotalExecutor-5, ExeutorCores-4, ExecutorMemory-25GB )\n",
    "\n",
    "1. Master node first create Driver in any worker node.\n",
    "2. Driver which is also known as Application Driver. Spark is writen in scala, and scala is a JVM process. Inside the Driver container it will create 2 main methods, one is for pyspark and another is for JVM.  Spark Core -> Java Wrapper -> Python Wrapper. JVM is called Application driver and pyspark is called pyspark driver.\n",
    "3. Then driver check the executor details and then it send the request to the resource manager. \n",
    "4. The resource manager sent request to node manager (worker), then it creates 5 executors in the ideal workers.\n",
    "5. Application driver send data and other details to the executors for the processing.\n",
    "6. All the excutors send computated result tot the driver.\n",
    "7. In the end all the container driver & exector will be delete. \n",
    "\n",
    "Note: Avoid writing/using UDF funtion in the pysaprk, it will require python worker in the executor container so it will impact on the performace. Always use buit-in function.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Schema\n",
    "\n",
    "There are 2 types of schema\n",
    "1. Using StructType & StructField\n",
    "    i. StructType: Defines structure of dataframe. List of StructField\n",
    "    ii. StructField: Define the column data type.\n",
    "    Example: StructType([StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)])\n",
    "2. Using DDL: In quotes comma seperated columns with data type in space.\n",
    "    Example: \"id integer, name string, age integer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_schema = StructType([StructField(\"airline\", StringType(), True), StructField(\"flight\", StringType(), True), \\\n",
    "                            StructField(\"source_city\", StringType(), True), StructField(\"departure_time\", StringType(), True), \\\n",
    "                            StructField(\"stops\", StringType(), True), StructField(\"arrival_time\", StringType(), True), \\\n",
    "                            StructField(\"destination_city\", StringType(), True), StructField(\"class\", StringType(), True), \\\n",
    "                            StructField(\"duration\", IntegerType(), True), StructField(\"days_left\", IntegerType(), True), StructField(\"price\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- source_city: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stops: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- days_left: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300153"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasformation & Action\n",
    "\n",
    "When action is called then only transformation is triggered.\n",
    "\n",
    "Transformation: Filter, Select, Union, Join, GroupBy, Distinct.\n",
    "\n",
    "Action: count, collect, show, read.\n",
    "\n",
    "Types Of Transformations\n",
    "1. Narrow -> Trasformation that doesn't require data movement within partition. Eg. Filter, Select, Union, Map\n",
    "2. Wide -> Trasformation that require datat movement within partition. Eg Join, GroupBy, Distinct. In wide transformation need to do data suffling between the partitions.\n",
    "\n",
    "Data Shuffling: Data is transferred through network among different partitions. Which is a very expensive trasformation.\n",
    "\n",
    "Note: When action is executed, the output data is collected by Driver. Driver memory should be always greater then the action output data. Otherwise there will be a memory error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65102"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\")    # Trasormation\n",
    "df1.count()     # Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG & Lazy Evaluation\n",
    "\n",
    "DAG -> Directed Acyclic Graph. It will never run loop, and execution happens in tree structure.\n",
    "\n",
    "On every action, a job is created and each job has it's own DAG.\n",
    "\n",
    "df = spark.read.format(\"csv\")\\          # Action\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\        # Action\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")       \n",
    "data_repartition = df.repartition(3)        # Wide Trasformation\n",
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\")        # Narrow Trasformation\n",
    "df = df.filter((Fun.col(\"destination_city\")==\"Mumbai\") | (Fun.col(\"destination_city\")==\"Delhi\"))        # Narrow Trasformation\n",
    "df = df.groupby(\"stops\").sum(\"price\")           # Wide Trasformation\n",
    "df.show()       # Action\n",
    "\n",
    "In the above code, after the execution it will create 4 jobs. 3 for actions and 1 for _______. DAG can be view from spark UI.\n",
    "1. Read -> Reading and generating java byte code. \n",
    "2. Inferschema -> Mapping\n",
    "3. Show\n",
    "\n",
    "Wide & Narrow trasformation will trigger, when action is defined. This is called Lazy Evaluation. Through Lazy Evaluation it also optimize the code, example in the above code it will merge the departure_time & destination_city filter to a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|      stops|sum(price)|\n",
      "+-----------+----------+\n",
      "|two_or_more|  26808235|\n",
      "|       zero| 195028713|\n",
      "|        one|2098748431|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "\n",
    "data_repartition = df.repartition(3)\n",
    "\n",
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\") \n",
    "\n",
    "df = df.filter((Fun.col(\"destination_city\")==\"Mumbai\") | (Fun.col(\"destination_city\")==\"Delhi\"))\n",
    "\n",
    "df = df.groupby(\"stops\").sum(\"price\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Engine\n",
    "\n",
    "Spark SQL Engine / Catayst Optimizer: \n",
    "\n",
    "SQL / Dataframe / Dataset -> Spark SQL Engine / Catayst Optimizer -> RDD Java Byte Code \n",
    "\n",
    "4 phases of Spark SQl Engine:\n",
    "1. Analysis : Linked with catalog. It checks whether table, columns or path is present or not. If not, then it will throw the \"Analysis Exception\". \n",
    "2. Optimized Logical Planning : In the Lazy evalution it performs the code optimization automatically. Example Merge multiple filters to single filter or During the Computation only 2 columns are required, so it will only pull the 2 column data automation for the optimation.  \n",
    "3. Physical Planning : Spark created multiple plans and among the best plan, it automatically choose the best one for the cost optimization. Example : Suppose there is 2 table one is big and other one is small. So it will broadcast the small table to avoid the shuffling. \n",
    "4. Code Generation\n",
    "\n",
    "<u>Code</u> --------> <u>Unresolved Logical Planning</u> ----Analysis----> <u>Resolved Logical Planning</u> ----Logical-Optimization----> <u>Optimized Logical Planning</u> --------> <u>Physical Planning</u> ----Cost-Model----> <u>Best Physical Plan</u> --------> <u>Final Code</u> \n",
    "                                                                        \n",
    "Catalog -> it is just a metadata of the data.                       \n",
    "Analysis Exception Error:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = spark.read.format(\"csv\")                .option(\"header\", \"true\")                .option(\"inferschema\", \"false\")                .schema(flight_schema)                .option(\"mode\", \"PERMISSIVE\")                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\\ndf.select(\"name1\").count()      # Column does not exsist so it will show the Analysis Error.\\n\\n'"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.select(\"name1\").count()      # Column does not exsist so it will show the Analysis Error.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD (Resilient Distributed Dataset)\n",
    "\n",
    "When \"Full Control On Data\" is required then RDD is used. RDD is a data structure in spark. Example: RDD distribute list in different node for the processing.\n",
    "\n",
    "Resilient -> In case of failure recovery. <BR>\n",
    "Destributed -> Data is over the cluster. <BR>\n",
    "Dataset -> Actual data over different partitions. <BR>\n",
    "\n",
    "RDD is immutable. From RDD1, RDD2 is creted. It means in the filteration it is creating new RDD i.e. RDD2 but it is not loosing unfiltered RDD1 data. <br>\n",
    "Example: <u>RDD1 (100 Rows Data)</u> --Filter-On-Age-Column--> <u>RDD2 (70 Rows Data)</u> --Filter-On-City-Column--> <u>RDD3 (30 Rows Data)</u> <br>\n",
    "Suppose RDD3 fails, then through DAG it knows how to create RDD3 from RDD2, so it will automatically create the new RDD3. This feature is also called as Fault Tolerance.  <br>\n",
    "\n",
    "Advantage:\n",
    "1. Best for unstructured data.\n",
    "2. It is tyoe safe. It will throw column error during the compile time but dataframe will through error during the run time (Run time - After 2 hrs of code execution it will through error in dataframe case).\n",
    "3. Flexibility & Control.\n",
    "\n",
    "\n",
    "Disadvantage: \n",
    "1. No optimization is done by spark. Developer need to write the optimization methods.\n",
    "2. In RDD it is \"How To\", and in Dataframe it is \"What To\".\n",
    "3. Very difficult to write code in RDD.\n",
    "\n",
    "| Method          | Code                                                                                          | \n",
    "|-----------------|-----------------------------------------------------------------------------------------------|\n",
    "| Dataframe       | data.groupBy(\"dept\").avg(\"age\")                                                               |\n",
    "| SQL             | SELECT dept, avg(age) from data group by dept                                                 |\n",
    "| RDD             | data.map {case dept, age} => dept -> (age, 1) <br /> .reduceByKey   { case ((a1,c1), (a2,c2)) => (a1+a2,c1+c2) } <br /> .map { case (dept, (age, c)) => dept -> age/c}                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "| Title | Data | Chart|\n",
    "|-------|------|------|\n",
    "| 1     | 2    | 3    |\n",
    "| 4     | 5    | 6    |\n",
    "| 7     | 8    | 9    |\n",
    "\n",
    "Types in which parquet is saved on disk:\n",
    "1. Columnar based file format. | 1 | 4 | 7 | 2 | 5 | 8 | 3 | 6 | 9 | <br>\n",
    "    Example: OLTP (Online Transactional Processing) Required when need to analysis on few columns like groupby, join \n",
    "2. Row based file format. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | => If we read specific index like 0 and 2, so everything time it need to jumps/skip the index 1. Due to this process will be slow. <br>\n",
    "    Example: OLAP (Online Analytical Processing) Required when need to update, insert, deletion is done on the date.\n",
    "\n",
    "Big Data -. Write Once & Read Many\n",
    "\n",
    "It is binary file format (Can be read with naked eyes). 3 main Advantages:\n",
    "1. Cost Reduce\n",
    "2. Time Reduce\n",
    "3. Performance Increase\n",
    "\n",
    "\n",
    "Data Encoding\n",
    "![Data Compression](/home/manish/Documents/VSCodeProjects/SparkTutorial/CompressionInParquet.png)\n",
    "\n",
    "Parquet Format:\n",
    "1. GZIP\n",
    "2. LZO\n",
    "3. Snappy\n",
    "\n",
    "Sample query executed, time taken by :-\n",
    "1. CSV is 2892 sec.\n",
    "2. LZO is 50 sec.\n",
    "3. GZIP is 40 sec.\n",
    "4. Snappy is 28 sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in Spark\n",
    "\n",
    "df.write.format(\"csv\")<br>\n",
    "        .option(\"header\", \"true\")<br>\n",
    "        .option(\"mode\", \"overwrite\")<br>\n",
    "        .option(\"path\", \"file_path\")<br>\n",
    "        .save() # Path can also be provided here also\n",
    "\n",
    "Type of wite modes:\n",
    "1. Append\n",
    "2. Overwrite\n",
    "3. errorIfExsists\n",
    "4. ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning & Bucketing\n",
    "\n",
    "Both the methods are for witing the data. Spark code performace increases when suck data queried.\n",
    "\n",
    "Partitioning: Create number of directors based on column categorical value. Like City, Gender\n",
    "\n",
    "Bucketing: Create number of directors is provided  by user, when column is continous value type. Like Age, ID \n",
    "\n",
    "Suppose in a backend 200 task are running, and in the end you are creating a bucket 5. Then it will create 200*5=1000 bucket. So we need to define repartition 5 in the code. like df.repartition(5)\n",
    "\n",
    "Bucket Suffling Eliminated: If need to join 2 tables which are saved in bucket. Then they should have:\n",
    "1. Same number of bucket count.\n",
    "2. Bucketing column name should be same.\n",
    "\n",
    "Bucket Pruning: It makes searching and joining faster.\n",
    "Example: 1234 5678 9102 > Aadhar card number and if you devide 123456789102/10000 then output will be 9102. it neams I need to search 9102 bucket number to get complete details of this aadhar card. This bucket could 2%-20% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf.write.format(\"csv\")        .option(\"header\", \"true\")        .option(\"mode\", \"overwrite\")        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/partition_destination_city\")        .partitionBy(\"destination_city\")        .save() \\n\\ndf.write.format(\"csv\")        .option(\"header\", \"true\")        .option(\"mode\", \"overwrite\")        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/bucket_duration\")        .bucketBy(3, \"duration\")        .saveAsTable(\"bucket_duration_flight\") \\n'"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"overwrite\")\\\n",
    "        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/partition_destination_city\")\\\n",
    "        .partitionBy(\"destination_city\")\\\n",
    "        .save() \n",
    "\n",
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"overwrite\")\\\n",
    "        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/bucket_duration\")\\\n",
    "        .bucketBy(3, \"duration\")\\\n",
    "        .saveAsTable(\"bucket_duration_flight\") \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ### Repartition & Coalesce Code\n"
     ]
    }
   ],
   "source": [
    "print(\"Add ### Repartition & Coalesce Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Jobs, Stages & Tasks\n",
    "\n",
    "Application: Code which executed through spark-submit. Single application is submitted at a time, mutiple application deployment is also feasible. <br>\n",
    "Job: In a application, total number of jobs is always equal to total number actions in your code. <br>\n",
    "Stages: Job is divided into stages, and stages is nothing but number of transformation in your code. When there is wide transformation the it split into next stage.<br> \n",
    "Tasks: Execute the code on the actual data. Triggered on executor.\n",
    "\n",
    "When job is created then minimum one stage is created in job and again minimum one task is created in stage. \n",
    "\n",
    "Sample Code: <br>\n",
    "from pyspark.sql.functions import *<br>\n",
    "spark = SparkSession.builder.master(\"Local[5]\").appName(\"Testing\").getorCreate()<br>\n",
    "employ_df = spark.read.format(\"csv\")<br>        # Action 1\n",
    "                 .option(\"header\", \"true\")<br>\n",
    "                 .load(\"path_to_file\")<br>\n",
    "print(employ_df.rdd.getNumpartitions())<br>\n",
    "employ_df = empoy_df.repartition(2)<br>         # Wide Trasformation 1\n",
    "print(employ_df.rdd.getNumpartitions())<br>\n",
    "employ_df = employ_df.filter(col(\"salary\")>90000)<br>       # Narrow Trasformation 1\n",
    "                     .select(\"id\", \"name\", \"age\", \"salary\")<br>         # Narrow Trasformation 2\n",
    "                     .groupby(\"age\").count()<br>    # Wide Trasformation 2\n",
    "employ_df.collect()<br>         # Action 2\n",
    "\n",
    "Code Flow: <br>\n",
    "Read => Repartion => Filter => Select => GroupBy => Collect <br>\n",
    "By default there are 200 partitions in wide transformation.\n",
    "\n",
    "Total Jobs: 2 Actions => 2 Jobs will be created.\n",
    "Total Stages : 2 Wide trasformation => 3 Stages will be created.\n",
    "Total tasks: Stage 1 (Restructure) 1 + Stage 2 (Read Exchange) 2 + Stage 3 (GroupBy) 200 => 203\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession & SparkContext\n",
    "\n",
    "Both provide entry to the spark cluster.\n",
    "\n",
    "In SparkSession, SQLSession, HiveSession etc. is encapsulated. Through SparkContext you need to create a context then only you can start using. \n",
    "\n",
    "RDD V/S DataFrame V/S Dataset\n",
    "\n",
    "| RDD                                     | DataFrame                              | Dataset                                                    |\n",
    "|-----------------------------------------|----------------------------------------|------------------------------------------------------------|\n",
    "| Fault Tolerant                          | Fault Tolerant                         | Fault Tolerant                                             |\n",
    "| Distributed                             | Distributed                            | Distributed                                                |\n",
    "| Immutable                               | Immutable                              | Immutable                                                  |\n",
    "| No Schema                               | Schema                                 | Schema                                                     |\n",
    "| Slow on Non-JVM Language                | Faster                                 | Faster                                                     |\n",
    "| No Execution optimization               | Optimization - Catalyst Optimizer      | Optimization                                               |\n",
    "| Low Level API                           | High Level API                         | High Level API                                             |\n",
    "| No SQL Support                          | SQL Support                            | SQL Support                                                |\n",
    "| Type Safe                               | No Type Safe                           | Type Safe                                                  |\n",
    "| Syntex error detected at compline time  | Syntex error detected at compline time | Syntex error detected at compline time                     |\n",
    "| Analysis error detected at compile time | Analysis error detected at run time    | Analysis error detected at compile time                    |\n",
    "| Java, Scala, Python, R                  | Java, Scala, Python, R                 | Java, Scala                                                |\n",
    "| High memory is used                     | High memory is used                    | Low memory is used. Tungstun encoder provide great benefit |\n",
    "\n",
    "Low & High level API means, that there function in high level for aggregation eg. sum, avg, select.<br>\n",
    "Time taken By Code:<br>\n",
    "![Time Taken By Code](/home/manish/Documents/VSCodeProjects/SparkTutorial/CodeRunTimeDistribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Functions\n",
    "\n",
    "1. Aliasing - Renaming a column in dataframe.\n",
    "2. Filter/Where - Filter dataframe.\n",
    "3. Literal - Add new column with same value in the complete dataframe.\n",
    "4. Adding New Columns - Adding new column.\n",
    "5. Renaming Columns - Renaming a column in dataframe.\n",
    "6. Casting Data Types - Converting datatype of a column.\n",
    "7. Removing Columns - Droping column from dataframe.\n",
    "\n",
    "Different Methods For Selecting Columns: <br>\n",
    "df.select(\"airline\", col(\"flight\"), df[\"source_city\"], df.departure_time)\n",
    "\n",
    "Experation Method: In this you can SQL functions<br>\n",
    "df.select(expr(\"price + 100\"))  => Without expr, price column will act as a String data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------+\n",
      "|airline_id|departure_time|days_left|\n",
      "+----------+--------------+---------+\n",
      "|  SpiceJet|       Evening|        1|\n",
      "|  SpiceJet| Early_Morning|        1|\n",
      "+----------+--------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|\n",
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "\n",
    "# Aliasing\n",
    "df.select(Fun.col(\"airline\").alias(\"airline_id\"), \"departure_time\", \"days_left\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8803|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        2| 5953|\n",
      "|SpiceJet|SG-8169|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        2| 5953|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8803|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        2| 5953|\n",
      "|SpiceJet|SG-8169|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        2| 5953|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8803|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        2| 5953|\n",
      "|SpiceJet|SG-8169|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        2| 5953|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter/Where\n",
    "df.filter(Fun.col(\"days_left\")>1).show(2)\n",
    "df.where(Fun.col(\"days_left\")>1).show(2)\n",
    "df.where((Fun.col(\"days_left\")>1) & (Fun.col(\"days_left\")<3)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+\n",
      "| airline| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|  data_type|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+\n",
      "|SpiceJet|SG-8709|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        1| 5953|flight_data|\n",
      "|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        1| 5953|flight_data|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Literal\n",
    "df.select(\"*\", Fun.lit(\"flight_data\").alias(\"data_type\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+--------------+\n",
      "| airline| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|   data_domain|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+--------------+\n",
      "|SpiceJet|SG-8709|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        1| 5953|airplanes_data|\n",
      "|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        1| 5953|airplanes_data|\n",
      "+--------+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding New Columns \n",
    "df.withColumn(\"data_domain\", Fun.lit(\"airplanes_data\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "| airline|flight_id|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+---------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|  SG-8709|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        1| 5953|\n",
      "|SpiceJet|  SG-8157|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        1| 5953|\n",
      "+--------+---------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "df.withColumnRenamed(\"flight\", \"flight_id\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- source_city: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stops: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- days_left: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casting Data Types\n",
    "df.withColumn(\"price\", Fun.col(\"price\").cast(\"integer\"))\\\n",
    "     .withColumn(\"days_left\", Fun.col(\"days_left\").cast(\"integer\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+\n",
      "| flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|\n",
      "+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+\n",
      "|SG-8709|      Delhi|       Evening| zero|       Night|          Mumbai|Economy|    null|        1|\n",
      "|SG-8157|      Delhi| Early_Morning| zero|     Morning|          Mumbai|Economy|    null|        1|\n",
      "+-------+-----------+--------------+-----+------------+----------------+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing Columns\n",
    "df.drop(\"price\", Fun.col(\"airline\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+-------------+---------+-----+\n",
      "|airline|flight|source_city|departure_time|stops|arrival_time|destination_city|  class|duration|days_left|price|  data_type|flight_cities|flight_id|price|\n",
      "+-------+------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+-------------+---------+-----+\n",
      "|Vistara|UK-953|      Delhi|         Night| zero|       Night|          Mumbai|Economy|    null|        2| 6060|flight_data| Delhi Mumbai|   UK-953| 6060|\n",
      "|Vistara|UK-995|      Delhi|       Morning| zero|   Afternoon|          Mumbai|Economy|    null|        2| 6375|flight_data| Delhi Mumbai|   UK-995| 6375|\n",
      "+-------+------+-----------+--------------+-----+------------+----------------+-------+--------+---------+-----+-----------+-------------+---------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL Query\n",
    "df.createOrReplaceTempView(\"flight\")\n",
    "spark.sql(\"\"\"\n",
    "     SELECT *, \"flight_data\" AS data_type, CONCAT(source_city, ' ',destination_city) AS flight_cities, flight AS flight_id, CAST(price as long) FROM flight WHERE days_left >= 2 AND price > 6000\n",
    "\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If Else \n",
    "\n",
    "When need to add new column, logical operations on dataframe columns.\n",
    "\n",
    "Most of the if else scenario are covered below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|  id|   name| age|salary|country|       dept|   adult|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|   1| manish|  26| 20000|  india|         IT|     Yes|\n",
      "|   2|  rahul|null| 40000|germany|engineering|Nonvalue|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|      No|\n",
      "|   4|roshini|  44|  null|     uk|engineering|     Yes|\n",
      "|   5|raushan|  35| 70000|  india|      sales|     Yes|\n",
      "|   6|   null|  29|200000|     uk|         IT|     Yes|\n",
      "|   7|   adam|  37| 65000|     us|         IT|     Yes|\n",
      "|   8|  chris|  16| 40000|     us|      sales|      No|\n",
      "|null|   null|null|  null|   null|       null|Nonvalue|\n",
      "|   7|   adam|  37| 65000|     us|         IT|     Yes|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,None,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(None,None,None,None,None,None),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"age\", \"salary\", \"country\", \"dept\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "emp_df.withColumn(\"adult\", Fun.when(Fun.col(\"age\")<18, \"No\")\n",
    "                                .when(Fun.col(\"age\")>18, \"Yes\")\n",
    "                                .otherwise(\"Nonvalue\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+------+-------+-----------+-----+\n",
      "|  id|   name|age|salary|country|       dept|adult|\n",
      "+----+-------+---+------+-------+-----------+-----+\n",
      "|   1| manish| 26| 20000|  india|         IT|  Yes|\n",
      "|   2|  rahul| 19| 40000|germany|engineering|  Yes|\n",
      "|   3|  pawan| 12| 60000|  india|      sales|   No|\n",
      "|   4|roshini| 44|  null|     uk|engineering|  Yes|\n",
      "|   5|raushan| 35| 70000|  india|      sales|  Yes|\n",
      "|   6|   null| 29|200000|     uk|         IT|  Yes|\n",
      "|   7|   adam| 37| 65000|     us|         IT|  Yes|\n",
      "|   8|  chris| 16| 40000|     us|      sales|   No|\n",
      "|null|   null| 19|  null|   null|       null|  Yes|\n",
      "|   7|   adam| 37| 65000|     us|         IT|  Yes|\n",
      "+----+-------+---+------+-------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"age\", Fun.when(Fun.col(\"age\").isNull(), Fun.lit(19))\n",
    "                  .otherwise(Fun.col(\"age\"))) \\\n",
    "        .withColumn(\"adult\", Fun.when(Fun.col(\"age\")>17, \"Yes\")\n",
    "                    .otherwise(\"No\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|  id|   name| age|salary|country|       dept|age_wise|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "|   1| manish|  26| 20000|  india|         IT|      No|\n",
      "|   2|  rahul|null| 40000|germany|engineering|    High|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|     Yes|\n",
      "|   4|roshini|  44|  null|     uk|engineering|      No|\n",
      "|   5|raushan|  35| 70000|  india|      sales|      No|\n",
      "|   6|   null|  29|200000|     uk|         IT|      No|\n",
      "|   7|   adam|  37| 65000|     us|         IT|      No|\n",
      "|   8|  chris|  16| 40000|     us|      sales|     Yes|\n",
      "|null|   null|null|  null|   null|       null|    High|\n",
      "|   7|   adam|  37| 65000|     us|         IT|      No|\n",
      "+----+-------+----+------+-------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"age_wise\", Fun.when((Fun.col(\"age\")>0) & (Fun.col(\"age\")<17), \"Yes\")\n",
    "                              .when((Fun.col(\"age\")>17) & (Fun.col(\"age\")<50), \"No\")\n",
    "                              .otherwise(\"High\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+-------+\n",
      "|  id|   name| age|salary|country|       dept|  adult|\n",
      "+----+-------+----+------+-------+-----------+-------+\n",
      "|   1| manish|  26| 20000|  india|         IT|  major|\n",
      "|   2|  rahul|null| 40000|germany|engineering|novalue|\n",
      "|   3|  pawan|  12| 60000|  india|      sales|  minor|\n",
      "|   4|roshini|  44|  null|     uk|engineering|  major|\n",
      "|   5|raushan|  35| 70000|  india|      sales|  major|\n",
      "|   6|   null|  29|200000|     uk|         IT|  major|\n",
      "|   7|   adam|  37| 65000|     us|         IT|  major|\n",
      "|   8|  chris|  16| 40000|     us|      sales|  minor|\n",
      "|null|   null|null|  null|   null|       null|novalue|\n",
      "|   7|   adam|  37| 65000|     us|         IT|  major|\n",
      "+----+-------+----+------+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *,\n",
    "          case when age<18 then 'minor'\n",
    "          when age>18 then 'major'\n",
    "          else 'novalue'\n",
    "          end as adult\n",
    "    FROM emp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique, Drop-Duplicates & Sorting\n",
    "\n",
    "Unique: Return the distinct values from the column or dataframe. Unique count might varies with different numbers of column collection.\n",
    "\n",
    "Sorting: It means arraning the column or dataframe. There are 2 types of sorting asending and desending. It can be done using single or multiple columns from the dataframe.\n",
    "\n",
    "Drop-Duplicates: Remove the rows from the dataframe, which are duplicates with respect to column value.\n",
    "\n",
    "Both the scenario examples covered below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Row Count :: 13\n",
      "Dataframe Distinct Row Count :: 10\n",
      "Dataframe Distinct Row Count Using Column:: 9\n",
      "Dataframe Remove Duplicates :: 10\n",
      "Dataframe Remove Duplicates Using Column :: 9\n",
      "Sorting in dataframe\n",
      "+---+------+------+----------+\n",
      "| id|  name|salary|manager_id|\n",
      "+---+------+------+----------+\n",
      "| 14| Priya| 90000|        18|\n",
      "| 16|Rajesh| 90000|        10|\n",
      "| 14| Priya| 80000|        18|\n",
      "| 11| Vikas| 75000|        16|\n",
      "| 18|   Sam| 65000|        17|\n",
      "| 18|   Sam| 65000|        17|\n",
      "| 13| Nidhi| 60000|        17|\n",
      "| 13| Nidhi| 60000|        17|\n",
      "| 17| Raman| 55000|        16|\n",
      "| 10|  Anil| 50000|        18|\n",
      "| 15| Mohit| 45000|        18|\n",
      "| 15| Mohit| 45000|        18|\n",
      "| 12| Nisha| 40000|        18|\n",
      "+---+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(10 ,'Anil',50000, 18),\n",
    "(11 ,'Vikas',75000,  16),\n",
    "(12 ,'Nisha',40000,  18),\n",
    "(13 ,'Nidhi',60000,  17),\n",
    "(14 ,'Priya',80000,  18),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(16 ,'Rajesh',90000, 10),\n",
    "(17 ,'Raman',55000, 16),\n",
    "(18 ,'Sam',65000,   17),\n",
    "(15 ,'Mohit',45000,  18),\n",
    "(13 ,'Nidhi',60000,  17),      \n",
    "(14 ,'Priya',90000,  18),  \n",
    "(18 ,'Sam',65000,   17)]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"salary\", \"manager_id\"]\n",
    "emp_df = spark.createDataFrame(data=data, schema=emp_schema);emp_df\n",
    "\n",
    "print(\"Dataframe Row Count ::\", emp_df.count())\n",
    "\n",
    "print(\"Dataframe Distinct Row Count ::\", emp_df.distinct().count())\n",
    "\n",
    "print(\"Dataframe Distinct Row Count Using Column::\", emp_df.select(\"name\").distinct().count())\n",
    "\n",
    "print(\"Dataframe Remove Duplicates ::\", emp_df.drop_duplicates().count())\n",
    "\n",
    "print(\"Dataframe Remove Duplicates Using Column ::\", emp_df.drop_duplicates([\"name\"]).count())\n",
    "\n",
    "print(\"Sorting in dataframe\")\n",
    "emp_df.sort(Fun.col(\"salary\").desc(), Fun.col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "1. Count\n",
    "2. Sum\n",
    "3. Max\n",
    "4. Min\n",
    "5. Min\n",
    "6. GroupBy\n",
    "\n",
    "Count: It is both action as well as trsformation. Count skip null values from single column.\n",
    "Example:\n",
    "df.count() => Action\n",
    "df.select(count(\"name\"))    => Trasformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Row Count :: 9\n",
      "Dataframe Row Count Using Column With Null ::\n",
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|        9|\n",
      "+---------+\n",
      "\n",
      "+---------+------------+----------+----------+-----------------+\n",
      "|row_count|total_salary|max_salary|min_salary|       avg_salary|\n",
      "+---------+------------+----------+----------+-----------------+\n",
      "|        9|      610000|    200000|     20000|67777.77777777778|\n",
      "+---------+------------+----------+----------+-----------------+\n",
      "\n",
      "+-----------+-------+-----------+\n",
      "|       dept|country|sum(salary)|\n",
      "+-----------+-------+-----------+\n",
      "|         IT|  india|      20000|\n",
      "|engineering|germany|      40000|\n",
      "|      sales|  india|     130000|\n",
      "|engineering|     uk|      50000|\n",
      "|         IT|     uk|     200000|\n",
      "|         IT|     us|     130000|\n",
      "|      sales|     us|      40000|\n",
      "+-----------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,50000,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"age\", \"salary\", \"country\", \"dept\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "print(\"Dataframe Row Count ::\", emp_df.count())\n",
    "\n",
    "print(\"Dataframe Row Count Using Column With Null ::\")\n",
    "emp_df.select(Fun.count(\"id\")).show()\n",
    "\n",
    "emp_df.select(Fun.count(\"salary\").alias(\"row_count\"), Fun.sum(\"salary\").alias(\"total_salary\"), Fun.max(\"salary\").alias(\"max_salary\"), Fun.min(\"salary\").alias(\"min_salary\"), Fun.avg(\"salary\").alias(\"avg_salary\")).show()\n",
    "\n",
    "emp_df.groupBy(\"dept\", \"country\").agg(Fun.sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Joins\n",
    "\n",
    "df1.join(df2, join_expression, join_type)\n",
    "\n",
    "1. Inner Join => Return all the matched rows.\n",
    "2. Outter Join => \n",
    "3. Left Join => Return all the matched rows + All the left table unmatched rows also.\n",
    "4. Right Join => Return all the matched rows + All the right table unmatched rows also.\n",
    "5. Left Semi Join => Distinct row or first row of the, all the matched rows + All the left table unmatched rows also.\n",
    "6. left Anti Join => \n",
    "7. Cross Join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "customer_df = spark.createDataFrame(data=customer_data, schema=customer_schema)\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "sales_df = spark.createDataFrame(data=sales_data, schema=sales_schema)\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "product_schema=['product_id','name','price']\n",
    "product_df = spark.createDataFrame(data=product_data, schema=product_schema)\n",
    "product_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"inner\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"inner\").count()\n",
    "#customer_df.join(sales_df, (customer_df[\"customer_id\"]==sales_df[\"customer_id\"]) & (customer_df[\"customer_id\"]==sales_df[\"customer_id\"]), \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n",
      "|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n",
      "|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"outer\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"outer\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n",
      "|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"right\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"right\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n",
      "|customer_id|customer_name|address|date_of_joining|\n",
      "+-----------+-------------+-------+---------------+\n",
      "|          1|       manish|  patna|     30-05-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|\n",
      "|          9|       ragini|kolkata|     03-03-2023|\n",
      "+-----------+-------------+-------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left_semi\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left_semi\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n",
      "|customer_id|customer_name|address|date_of_joining|\n",
      "+-----------+-------------+-------+---------------+\n",
      "|          3|       nikita|  delhi|     25-06-2023|\n",
      "|          4|        rahul| ranchi|     24-03-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|\n",
      "|          8|      prakash| ranchi|     24-02-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|\n",
      "+-----------+-------------+-------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left_anti\").show()\n",
    "customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"left_anti\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          2|         5|       3|      01-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          5|        22|       1|      22-03-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          7|        22|       4|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          9|         5|       6|      03-03-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|         11|        12|      76|      12-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        22|      10|      01-06-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          7|        22|       4|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        56|       2|      25-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|         11|        12|      76|      12-03-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).show()\n",
    "customer_df.crossJoin(sales_df).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies In Spark\n",
    "\n",
    "1. Shuffle sort-merge join => Before joining the table it will sort both the table. All the computation happens in CPU.\n",
    "2. Shuffle hash join => It will convert the small table in hash table, and this happen in-memory. During joining, it will convert the bigger table id into hash and then it serch in smaller hash table.   \n",
    "3. Broadcast hash join => This will broadcast the smaller table in all the executor and this process is done by driver. By default broadcast table size is 10 MB, it can be updated using \"spark.conf.set('spark.sql.autoBroadcastJoinThreshold', 20455780)\" \n",
    "4. Cartesian join\n",
    "5. Broadcast nested loop join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/08 00:15:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [customer_id#23708L], [customer_id#23716L], Inner\n",
      "   :- Sort [customer_id#23708L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#23708L, 5), ENSURE_REQUIREMENTS, [plan_id=21294]\n",
      "   :     +- Filter isnotnull(customer_id#23708L)\n",
      "   :        +- Scan ExistingRDD[customer_id#23708L,customer_name#23709,address#23710,date_of_joining#23711]\n",
      "   +- Sort [customer_id#23716L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#23716L, 5), ENSURE_REQUIREMENTS, [plan_id=21295]\n",
      "         +- Filter isnotnull(customer_id#23716L)\n",
      "            +- Scan ExistingRDD[customer_id#23716L,product_id#23717L,quantity#23718L,date_of_purchase#23719]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[5]\").appName(\"SparkSumbittest\").getOrCreate()\n",
    "\n",
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "customer_df = spark.createDataFrame(data=customer_data, schema=customer_schema)\n",
    "customer_df.createOrReplaceTempView(\"customer\")\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "sales_df = spark.createDataFrame(data=sales_data, schema=sales_schema)\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "product_schema=['product_id','name','price']\n",
    "product_df = spark.createDataFrame(data=product_data, schema=product_schema)\n",
    "product_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "sort_merge_df = customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"inner\")\n",
    "sort_merge_df.show()\n",
    "sort_merge_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [customer_id#23708L], [customer_id#23716L], Inner\n",
      "   :- Sort [customer_id#23708L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(customer_id#23708L, 5), ENSURE_REQUIREMENTS, [plan_id=21443]\n",
      "   :     +- Filter isnotnull(customer_id#23708L)\n",
      "   :        +- Scan ExistingRDD[customer_id#23708L,customer_name#23709,address#23710,date_of_joining#23711]\n",
      "   +- Sort [customer_id#23716L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(customer_id#23716L, 5), ENSURE_REQUIREMENTS, [plan_id=21444]\n",
      "         +- Filter isnotnull(customer_id#23716L)\n",
      "            +- Scan ExistingRDD[customer_id#23716L,product_id#23717L,quantity#23718L,date_of_purchase#23719]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "sort_merge_df = customer_df.join(sales_df, customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"inner\")\n",
    "sort_merge_df.show()\n",
    "sort_merge_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [customer_id#23708L], [customer_id#23716L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(customer_id#23708L)\n",
      "   :  +- Scan ExistingRDD[customer_id#23708L,customer_name#23709,address#23710,date_of_joining#23711]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=21531]\n",
      "      +- Filter isnotnull(customer_id#23716L)\n",
      "         +- Scan ExistingRDD[customer_id#23716L,product_id#23717L,quantity#23718L,date_of_purchase#23719]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_merge_df = customer_df.join(Fun.broadcast(sales_df), customer_df[\"customer_id\"]==sales_df[\"customer_id\"], \"inner\")\n",
    "sort_merge_df.show()\n",
    "sort_merge_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function\n",
    "\n",
    "Window functions operate on a group of rows, referred to as a window, and calculate a return value for each row based on the group of rows.\n",
    "\n",
    "1. Row Number => Row Number assign distinct id to each row.\n",
    "2. Rank => Rows which has same value, it keeps the same id's for that row but skip the number when it repeats.\n",
    "3. Dense Rank => Rows which has same value, it keeps the same id's for that row. \n",
    "\n",
    "Example\n",
    "| Salary | Row Number | Rank | Dense Rank |\n",
    "|--------|------------|------|------------|\n",
    "| 50000  | 1          | 1    | 1          |\n",
    "| 60000  | 2          | 2    | 2          |\n",
    "| 70000  | 3          | 3    | 3          |\n",
    "| 80000  | 4          | 4    | 4          |\n",
    "| 80000  | 5          | 4    | 4          |\n",
    "| 80000  | 6          | 4    | 4          |\n",
    "| 90000  | 7          | 7    | 5          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "|id |name   |age |salary|country|dept       |row_number|rank|dense_rank|\n",
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "|6  |null   |29  |200000|uk     |IT         |1         |1   |1         |\n",
      "|7  |adam   |37  |65000 |us     |IT         |2         |2   |2         |\n",
      "|7  |adam   |37  |65000 |us     |IT         |3         |2   |2         |\n",
      "|1  |manish |26  |20000 |india  |IT         |4         |4   |3         |\n",
      "|4  |roshini|44  |50000 |uk     |engineering|1         |1   |1         |\n",
      "|2  |rahul  |null|40000 |germany|engineering|2         |2   |2         |\n",
      "|5  |raushan|35  |70000 |india  |sales      |1         |1   |1         |\n",
      "|3  |pawan  |12  |60000 |india  |sales      |2         |2   |2         |\n",
      "|8  |chris  |16  |40000 |us     |sales      |3         |3   |3         |\n",
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "emp_data = [\n",
    "(1,'manish',26,20000,'india','IT'),\n",
    "(2,'rahul',None,40000,'germany','engineering'),\n",
    "(3,'pawan',12,60000,'india','sales'),\n",
    "(4,'roshini',44,50000,'uk','engineering'),\n",
    "(5,'raushan',35,70000,'india','sales'),\n",
    "(6,None,29,200000,'uk','IT'),\n",
    "(7,'adam',37,65000,'us','IT'),\n",
    "(8,'chris',16,40000,'us','sales'),\n",
    "(7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"age\", \"salary\", \"country\", \"dept\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "win_main = Window.partitionBy(\"dept\").orderBy(Fun.desc(\"salary\"))\n",
    "emp_df.withColumn(\"row_number\", Fun.row_number().over(win_main))\\\n",
    "        .withColumn(\"rank\", Fun.rank().over(win_main))\\\n",
    "        .withColumn(\"dense_rank\", Fun.dense_rank().over(win_main))\\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "|id |name   |age |salary|country|dept       |row_number|rank|dense_rank|\n",
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "|6  |null   |29  |200000|uk     |IT         |1         |1   |1         |\n",
      "|7  |adam   |37  |65000 |us     |IT         |2         |2   |2         |\n",
      "|7  |adam   |37  |65000 |us     |IT         |3         |2   |2         |\n",
      "|4  |roshini|44  |50000 |uk     |engineering|1         |1   |1         |\n",
      "|2  |rahul  |null|40000 |germany|engineering|2         |2   |2         |\n",
      "|5  |raushan|35  |70000 |india  |sales      |1         |1   |1         |\n",
      "|3  |pawan  |12  |60000 |india  |sales      |2         |2   |2         |\n",
      "+---+-------+----+------+-------+-----------+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn(\"row_number\", Fun.row_number().over(win_main))\\\n",
    "        .withColumn(\"rank\", Fun.rank().over(win_main))\\\n",
    "        .withColumn(\"dense_rank\", Fun.dense_rank().over(win_main))\\\n",
    "        .filter(Fun.col(\"dense_rank\")<=2)\\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead & Lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
