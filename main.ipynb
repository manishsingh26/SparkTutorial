{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hadoop V/S Spark\n",
    "\n",
    "| Parameter       | Hadoop                                                                                        | Spark                                                                                                         |\n",
    "|-----------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n",
    "| Performance     | Hadoop is slower then spark. it writes data back to the disk and read again from to in-memory | Spark is faster then hadoop because spark do all the the computation in memory.                               |\n",
    "| Batch/Streaming | Build for batch data processing.                                                              | Build for batch as well as streaming data processing.                                                         |\n",
    "| Ease Of Use     | Difficult to write code in hadoop. Hive was built to make it easier                           | Easy to write and debug code. Interactive shell to develop and test. Spark provides high and low level API's. |\n",
    "| Security        | Use kerberos Authentication and ACL autirization. (YARN)                                      | Don't have solid security. (HDFS->ACL)(YARN->kerberos).                                                       |\n",
    "| Fault Talerance | It has block of data (128 MB) and replication factor to handle the failure.                   | Use DAG to provide fault talerance (DAG).                                                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV Data In Spark\n",
    "1. Format (Optional) -> CSV, JSON, JDBC/ODBC, Table, parquate. By default it takes parquate as read method.\n",
    "2. Option (Optional) -> InferSchema, Mode, header.\n",
    "3. Schema (Optional) -> Custom schema can be used.\n",
    "4. Load -> File path.\n",
    "\n",
    "Read Mode:\n",
    "1. Failfest: Fail execution if malformed record in dataset.\n",
    "2. Dropmalformed: Drop the corrupted record.\n",
    "3. Permissive: Default mode. Set null values to all the corrupted fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql import functions as Fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkSumbittest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "| airline| flight|source_city|departure_time|stops| arrival_time|destination_city|  class|duration|days_left|price|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "|SpiceJet|SG-8709|      Delhi|       Evening| zero|        Night|          Mumbai|Economy|    2.17|        1| 5953|\n",
      "|SpiceJet|SG-8157|      Delhi| Early_Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5953|\n",
      "| AirAsia| I5-764|      Delhi| Early_Morning| zero|Early_Morning|          Mumbai|Economy|    2.17|        1| 5956|\n",
      "| Vistara| UK-995|      Delhi|       Morning| zero|    Afternoon|          Mumbai|Economy|    2.25|        1| 5955|\n",
      "| Vistara| UK-963|      Delhi|       Morning| zero|      Morning|          Mumbai|Economy|    2.33|        1| 5955|\n",
      "+--------+-------+-----------+--------------+-----+-------------+----------------+-------+--------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"true\")\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Job Submit \n",
    "\n",
    "(Assumptions DriverMemory-20GB, TotalExecutor-5, ExeutorCores-4, ExecutorMemory-25GB )\n",
    "\n",
    "1. Master node first create Driver in any worker node.\n",
    "2. Driver which is also known as Application Driver. Spark is writen in scala, and scala is a JVM process. Inside the Driver container it will create 2 main methods, one is for pyspark and another is for JVM.  Spark Core -> Java Wrapper -> Python Wrapper. JVM is called Application driver and pyspark is called pyspark driver.\n",
    "3. Then driver check the executor details and then it send the request to the resource manager. \n",
    "4. The resource manager sent request to node manager (worker), then it creates 5 executors in the ideal workers.\n",
    "5. Application driver send data and other details to the executors for the processing.\n",
    "6. All the excutors send computated result tot the driver.\n",
    "7. In the end all the container driver & exector will be delete. \n",
    "\n",
    "Note: Avoid writing/using UDF funtion in the pysaprk, it will require python worker in the executor container so it will impact on the performace. Always use buit-in function.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Schema\n",
    "\n",
    "There are 2 types of schema\n",
    "1. Using StructType & StructField\n",
    "    i. StructType: Defines structure of dataframe. List of StructField\n",
    "    ii. StructField: Define the column data type.\n",
    "    Example: StructType([StructField(\"id\", IntegerType(), True), StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)])\n",
    "2. Using DDL: In quotes comma seperated columns with data type in space.\n",
    "    Example: \"id integer, name string, age integer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_schema = StructType([StructField(\"airline\", StringType(), True), StructField(\"flight\", StringType(), True), \\\n",
    "                            StructField(\"source_city\", StringType(), True), StructField(\"departure_time\", StringType(), True), \\\n",
    "                            StructField(\"stops\", StringType(), True), StructField(\"arrival_time\", StringType(), True), \\\n",
    "                            StructField(\"destination_city\", StringType(), True), StructField(\"class\", StringType(), True), \\\n",
    "                            StructField(\"duration\", IntegerType(), True), StructField(\"days_left\", IntegerType(), True), StructField(\"price\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airline: string (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      " |-- source_city: string (nullable = true)\n",
      " |-- departure_time: string (nullable = true)\n",
      " |-- stops: string (nullable = true)\n",
      " |-- arrival_time: string (nullable = true)\n",
      " |-- destination_city: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- days_left: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300153"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasformation & Action\n",
    "\n",
    "When action is called then only transformation is triggered.\n",
    "\n",
    "Transformation: Filter, Select, Union, Join, GroupBy, Distinct.\n",
    "\n",
    "Action: count, collect, show, read.\n",
    "\n",
    "Types Of Transformations\n",
    "1. Narrow -> Trasformation that doesn't require data movement within partition. Eg. Filter, Select, Union, Map\n",
    "2. Wide -> Trasformation that require datat movement within partition. Eg Join, GroupBy, Distinct. In wide transformation need to do data suffling between the partitions.\n",
    "\n",
    "Data Shuffling: Data is transferred through network among different partitions. Which is a very expensive trasformation.\n",
    "\n",
    "Note: When action is executed, the output data is collected by Driver. Driver memory should be always greater then the action output data. Otherwise there will be a memory error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65102"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\")    # Trasormation\n",
    "df1.count()     # Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAG & Lazy Evaluation\n",
    "\n",
    "DAG -> Directed Acyclic Graph. It will never run loop, and execution happens in tree structure.\n",
    "\n",
    "On every action, a job is created and each job has it's own DAG.\n",
    "\n",
    "df = spark.read.format(\"csv\")\\          # Action\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\        # Action\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")       \n",
    "data_repartition = df.repartition(3)        # Wide Trasformation\n",
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\")        # Narrow Trasformation\n",
    "df = df.filter((Fun.col(\"destination_city\")==\"Mumbai\") | (Fun.col(\"destination_city\")==\"Delhi\"))        # Narrow Trasformation\n",
    "df = df.groupby(\"stops\").sum(\"price\")           # Wide Trasformation\n",
    "df.show()       # Action\n",
    "\n",
    "In the above code, after the execution it will create 4 jobs. 3 for actions and 1 for _______. DAG can be view from spark UI.\n",
    "1. Read -> Reading and generating java byte code. \n",
    "2. Inferschema -> Mapping\n",
    "3. Show\n",
    "\n",
    "Wide & Narrow trasformation will trigger, when action is defined. This is called Lazy Evaluation. Through Lazy Evaluation it also optimize the code, example in the above code it will merge the departure_time & destination_city filter to a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|      stops|sum(price)|\n",
      "+-----------+----------+\n",
      "|two_or_more|  26808235|\n",
      "|        one|2098748431|\n",
      "|       zero| 195028713|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "\n",
    "data_repartition = df.repartition(3)\n",
    "\n",
    "df1 = df.filter(df[\"departure_time\"]==\"Evening\") \n",
    "\n",
    "df = df.filter((Fun.col(\"destination_city\")==\"Mumbai\") | (Fun.col(\"destination_city\")==\"Delhi\"))\n",
    "\n",
    "df = df.groupby(\"stops\").sum(\"price\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Engine\n",
    "\n",
    "Spark SQL Engine / Catayst Optimizer: \n",
    "\n",
    "SQL / Dataframe / Dataset -> Spark SQL Engine / Catayst Optimizer -> RDD Java Byte Code \n",
    "\n",
    "4 phases of Spark SQl Engine:\n",
    "1. Analysis : Linked with catalog. It checks whether table, columns or path is present or not. If not, then it will throw the \"Analysis Exception\". \n",
    "2. Optimized Logical Planning : In the Lazy evalution it performs the code optimization automatically. Example Merge multiple filters to single filter or During the Computation only 2 columns are required, so it will only pull the 2 column data automation for the optimation.  \n",
    "3. Physical Planning : Spark created multiple plans and among the best plan, it automatically choose the best one for the cost optimization. Example : Suppose there is 2 table one is big and other one is small. So it will broadcast the small table to avoid the shuffling. \n",
    "4. Code Generation\n",
    "\n",
    "<u>Code</u> --------> <u>Unresolved Logical Planning</u> ----Analysis----> <u>Resolved Logical Planning</u> ----Logical-Optimization----> <u>Optimized Logical Planning</u> --------> <u>Physical Planning</u> ----Cost-Model----> <u>Best Physical Plan</u> --------> <u>Final Code</u> \n",
    "                                                                        \n",
    "Catalog -> it is just a metadata of the data.                       \n",
    "Analysis Exception Error:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `name1` cannot be resolved. Did you mean one of the following? [`class`, `price`, `stops`, `flight`, `airline`].;\n'Project ['name1]\n+- Relation [airline#435,flight#436,source_city#437,departure_time#438,stops#439,arrival_time#440,destination_city#441,class#442,duration#443,days_left#444,price#445] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      2\u001b[0m                 \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      3\u001b[0m                 \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39minferschema\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m                 \u001b[39m.\u001b[39mschema(flight_schema)\\\n\u001b[1;32m      5\u001b[0m                 \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPERMISSIVE\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      6\u001b[0m                 \u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m df\u001b[39m.\u001b[39;49mselect(\u001b[39m\"\u001b[39;49m\u001b[39mname1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mcount()      \u001b[39m# Column does not exsist so it will show the Analysis Error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2993\u001b[0m \n\u001b[1;32m   2994\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[39m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3036\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   3037\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `name1` cannot be resolved. Did you mean one of the following? [`class`, `price`, `stops`, `flight`, `airline`].;\n'Project ['name1]\n+- Relation [airline#435,flight#436,source_city#437,departure_time#438,stops#439,arrival_time#440,destination_city#441,class#442,duration#443,days_left#444,price#445] csv\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferschema\", \"false\")\\\n",
    "                .schema(flight_schema)\\\n",
    "                .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                .load(\"/home/manish/Documents/VSCodeProjects/SparkTutorial/flight_data.csv\")\n",
    "df.select(\"name1\").count()      # Column does not exsist so it will show the Analysis Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD (Resilient Distributed Dataset)\n",
    "\n",
    "When \"Full Control On Data\" is required then RDD is used. RDD is a data structure in spark. Example: RDD distribute list in different node for the processing.\n",
    "\n",
    "Resilient -> In case of failure recovery. <BR>\n",
    "Destributed -> Data is over the cluster. <BR>\n",
    "Dataset -> Actual data over different partitions. <BR>\n",
    "\n",
    "RDD is immutable. From RDD1, RDD2 is creted. It means in the filteration it is creating new RDD i.e. RDD2 but it is not loosing unfiltered RDD1 data. <br>\n",
    "Example: <u>RDD1 (100 Rows Data)</u> --Filter-On-Age-Column--> <u>RDD2 (70 Rows Data)</u> --Filter-On-City-Column--> <u>RDD3 (30 Rows Data)</u> <br>\n",
    "Suppose RDD3 fails, then through DAG it knows how to create RDD3 from RDD2, so it will automatically create the new RDD3. This feature is also called as Fault Tolerance.  <br>\n",
    "\n",
    "Advantage:\n",
    "1. Best for unstructured data.\n",
    "2. It is tyoe safe. It will throw column error during the compile time but dataframe will through error during the run time (Run time - After 2 hrs of code execution it will through error in dataframe case).\n",
    "3. Flexibility & Control.\n",
    "\n",
    "\n",
    "Disadvantage: \n",
    "1. No optimization is done by spark. Developer need to write the optimization methods.\n",
    "2. In RDD it is \"How To\", and in Dataframe it is \"What To\".\n",
    "3. Very difficult to write code in RDD.\n",
    "\n",
    "| Method          | Code                                                                                          | \n",
    "|-----------------|-----------------------------------------------------------------------------------------------|\n",
    "| Dataframe       | data.groupBy(\"dept\").avg(\"age\")                                                               |\n",
    "| SQL             | SELECT dept, avg(age) from data group by dept                                                 |\n",
    "| RDD             | data.map {case dept, age} => dept -> (age, 1) <br /> .reduceByKey   { case ((a1,c1), (a2,c2)) => (a1+a2,c1+c2) } <br /> .map { case (dept, (age, c)) => dept -> age/c}                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "| Title | Data | Chart|\n",
    "|-------|------|------|\n",
    "| 1     | 2    | 3    |\n",
    "| 4     | 5    | 6    |\n",
    "| 7     | 8    | 9    |\n",
    "\n",
    "Types in which parquet is saved on disk:\n",
    "1. Columnar based file format. | 1 | 4 | 7 | 2 | 5 | 8 | 3 | 6 | 9 | <br>\n",
    "    Example: OLTP (Online Transactional Processing) Required when need to analysis on few columns like groupby, join \n",
    "2. Row based file format. | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | => If we read specific index like 0 and 2, so everything time it need to jumps/skip the index 1. Due to this process will be slow. <br>\n",
    "    Example: OLAP (Online Analytical Processing) Required when need to update, insert, deletion is done on the date.\n",
    "\n",
    "Big Data -. Write Once & Read Many\n",
    "\n",
    "It is binary file format (Can be read with naked eyes). 3 main Advantages:\n",
    "1. Cost Reduce\n",
    "2. Time Reduce\n",
    "3. Performance Increase\n",
    "\n",
    "\n",
    "Data Encoding\n",
    "![Data Compression](/home/manish/Documents/VSCodeProjects/SparkTutorial/CompressionInParquet.png)\n",
    "\n",
    "Parquet Format:\n",
    "1. GZIP\n",
    "2. LZO\n",
    "3. Snappy\n",
    "\n",
    "Sample query executed, time taken by :-\n",
    "1. CSV is 2892 sec.\n",
    "2. LZO is 50 sec.\n",
    "3. GZIP is 40 sec.\n",
    "4. Snappy is 28 sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in Spark\n",
    "\n",
    "df.write.format(\"csv\")<br>\n",
    "        .option(\"header\", \"true\")<br>\n",
    "        .option(\"mode\", \"overwrite\")<br>\n",
    "        .option(\"path\", \"file_path\")<br>\n",
    "        .save() # Path can also be provided here also\n",
    "\n",
    "Type of wite modes:\n",
    "1. Append\n",
    "2. Overwrite\n",
    "3. errorIfExsists\n",
    "4. ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning & Bucketing\n",
    "\n",
    "Both the methods are for witing the data. Spark code performace increases when suck data queried.\n",
    "\n",
    "Partitioning: Create number of directors based on column categorical value. Like City, Gender\n",
    "\n",
    "Bucketing: Create number of directors is provided  by user, when column is continous value type. Like Age, ID \n",
    "\n",
    "Suppose in a backend 200 task are running, and in the end you are creating a bucket 5. Then it will create 200*5=1000 bucket. So we need to define repartition 5 in the code. like df.repartition(5)\n",
    "\n",
    "Bucket Suffling Eliminated: If need to join 2 tables which are saved in bucket. Then they should have:\n",
    "1. Same number of bucket count.\n",
    "2. Bucketing column name should be same.\n",
    "\n",
    "Bucket Pruning: It makes searching and joining faster.\n",
    "Example: 1234 5678 9102 > Aadhar card number and if you devide 123456789102/10000 then output will be 9102. it neams I need to search 9102 bucket number to get complete details of this aadhar card. This bucket could 2%-20% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/04 01:41:55 WARN ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"overwrite\")\\\n",
    "        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/partition_destination_city\")\\\n",
    "        .partitionBy(\"destination_city\")\\\n",
    "        .save() \n",
    "\n",
    "df.write.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"overwrite\")\\\n",
    "        .option(\"path\", \"/home/manish/Documents/VSCodeProjects/SparkTutorial/bucket_duration\")\\\n",
    "        .bucketBy(3, \"duration\")\\\n",
    "        .saveAsTable(\"bucket_duration_flight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Jobs, Stages & Taks\n",
    "\n",
    "Application: Code which executed through spark-submit. Single application is submitted at a time, mutiple application deployment is also feasible. <br>\n",
    "Job: In a application, total number of jobs is always equal to total number actions in your code. <br>\n",
    "Stages: Job is divided into stages, and stages is nothing but number of transformation in your code. <br> \n",
    "Tasks: Execute the code on the actual data. Triggered on executor.\n",
    "\n",
    "When job is created then minimum one stage is created in job and again minimum one task is created in stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
